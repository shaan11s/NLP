{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMO6uuM+3+/K1OqsiZ9PQJ3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"ka7ypgfujOJC","executionInfo":{"status":"error","timestamp":1714105430519,"user_tz":300,"elapsed":1781,"user":{"displayName":"Shaan S (Shaan)","userId":"04069847802196147798"}},"outputId":"d1b34e89-b053-478a-c28b-1b0bd115f086"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 1 is not in range","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cc0f46b57e57>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mK_List\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mperform_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-cc0f46b57e57>\u001b[0m in \u001b[0;36mperform_kmeans\u001b[0;34m(tweet_ds, K, centroids)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m#print(\"centroid {} is {}\".format(i,centroids[i]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 1"]}],"source":["import re\n","import pandas as pd\n","import numpy as np\n","import random\n","\n","\n","#DONE\n","def data_cleaning(text):\n","    # Split the text into words and process each word\n","    words = text.split()\n","    cleaned_words = [\n","        re.sub(r\"^#(.+)\", lambda m: m.group(1).lower(), word) if word.startswith('#')\n","        else word.lower().strip(\"'\")\n","        for word in words if not word.startswith('@') and not word.startswith('http://')\n","    ]\n","    return cleaned_words\n","\n","#DONE\n","def jaccard_distance(set1,set2):\n","    set1 = set(set1)\n","    set2 = set(set2)\n","    # intersection of two sets\n","    intersection = len(set1.intersection(set2))\n","    # Unions of two sets\n","    union = len(set1.union(set2))\n","    return 1 - intersection / union\n","\n","#DONE\n","def preprocess_data(source_file):\n","    data_frame = pd.read_csv(source_file, encoding=\"ISO-8859-1\", header=None, sep=\"|\")\n","\n","    # Randomly shuffle the dataframe rows\n","    shuffled_data = data_frame.sample(frac=1).reset_index(drop=True)\n","\n","    # Retain only the relevant column (assuming the third column contains the tweet text)\n","    tweets = shuffled_data.iloc[:, 2]\n","\n","    cleaned_tweets = tweets.apply(data_cleaning)\n","    final_data = cleaned_tweets.dropna()\n","\n","    return final_data\n","\n","\n","def perform_kmeans(tweet_ds,K,centroids=None):\n","    tweet_ds = tweet_ds.sample(frac=1).reset_index(drop=True)\n","    #Initializing the Centroids for the the first Iteration\n","    if centroids==None:\n","        centroids = {}\n","        for i in range(K):\n","            if(tweet_ds[i] not in list(centroids.keys())):\n","                centroids[i] = tweet_ds[i]\n","                #print(\"centroid {} is {}\".format(i,centroids[i]))\n","    tweet_cluster = {i:[] for i in range(K)}\n","    #Assignment step : Clustering the tweets to the centroids\n","    for tweet in tweet_ds:\n","        tweet_distance = [jaccard_distance(tweet,centroids[c]) for c in centroids]\n","        min_distance = tweet_distance.index(min(tweet_distance))\n","        tweet_cluster[min_distance].append(tweet)\n","    new_centroid = update_centroid(tweet_cluster,K)\n","    converge = False\n","    centroids_tweet = list(centroids.values())\n","    new_centroids_tweet = list(new_centroid.values())\n","    #Converging check - Check if the old_centroid and updated new centroids are equal\n","    for i in range(K):\n","        if(centroids_tweet[i] != new_centroids_tweet[i]):\n","            converge = False\n","            break\n","        else:\n","            converge = True\n","    if converge == False:\n","        print(\"Not Converged...Recomputing the Centroid\")\n","        centroids = new_centroid.copy()\n","        perform_kmeans(tweet_ds,K,centroids)\n","    else:\n","        print(\"Converge Succeed\")\n","        sse_total = compute_ss_error(tweet_cluster,centroids)\n","        print(\"\\nThe Sum of Squared Error is \",sse_total)\n","        #for i in range(K):\n","            #print(\"\\nThe number of tweets in the cluster {0} is {1} \".format(i+1,len(tweet_cluster[i])))\n","\n","            #print(\"\\n{0} : {1} \".format(i+1,len(tweet_cluster[i])))\n","\n","#DONE\n","def update_centroid(tweet_cluster, K):\n","    updated_centroid = {}\n","\n","    for cluster_id, cluster_tweets in tweet_cluster.items():\n","        if not cluster_tweets:  # Skip empty clusters\n","            continue\n","\n","        inter_cluster_dist = []\n","\n","        for tweet in cluster_tweets:\n","            tweet_distance = [jaccard_distance(tweet, c) for c in cluster_tweets]\n","            inter_total_dist = sum(tweet_distance)\n","            inter_cluster_dist.append(inter_total_dist)\n","\n","        cluster_tweet_index = inter_cluster_dist.index(min(inter_cluster_dist))\n","        updated_centroid[cluster_id] = cluster_tweets[cluster_tweet_index]\n","\n","    return updated_centroid\n","\n","#DONE\n","def compute_ss_error(tweet_cluster, centroids):\n","    return sum(jaccard_distance(centroids[centroid_id], tweet)**2\n","               for centroid_id, tweets in tweet_cluster.items()\n","               for tweet in tweets)\n","\n","#INPUTS\n","dataset = \"/content/sample_data/foxnewshealth.txt\"\n","K_List = [15, 20, 25, 30, 70] #cluster sizes\n","\n","#Processing the dataset and removing the delimiters\n","f = open(dataset,\"r+\", encoding=\"ISO-8859-1\")\n","line =f.readlines()\n","f.truncate(0)\n","f.seek(0)\n","for i in range (0,len(line)):\n","    if(line[i].count('|') != 2):\n","        line_split = line[i].split('|')\n","        new_line = '|'.join(line_split[:3]) + ' ' + ' '.join(line_split[3:])\n","        line[i] = new_line\n","for j in range(0,len(line)):\n","    l = line[j].strip()\n","    f.write(line[j])\n","f.close()\n","cleaned_data = preprocess_data(dataset)\n","\n","for K in K_List:\n","  perform_kmeans(cleaned_data,K,centroids = None)"]}]}